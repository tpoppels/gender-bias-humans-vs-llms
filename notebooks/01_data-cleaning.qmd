---
title: "Data Cleaning and Preprocessing"
author: "Till Poppels"
format: 
  html:
    theme: cosmo
    toc: true
    toc-depth: 2
    number-sections: true
execute:
  echo: true
  warning: false
  message: false
  error: true
  cache: false
---

```{r setup}
#| label: setup
#| include: false

library(tidyverse)
library(here)
```

# Overview

The raw data files (gitignored for participant privacy) are Ibex results files and as such don't have a consistent CSV structure (different parts of the results files contain different numbers of columns). We will essentially grep the lines for different types of data and store them in individual data frames that will eventually get glued together into a single dataframe and stored as `data/processed/clean-data.rds`.

We will focus here on the data needed for the main research questions and hypothesis tests:

1.  Whether human readers were surprised to see "she" refer to the next Vice President *before the 2020 election*
2.  Whether those expectations were reversed *after the 2020 election*, which saw Kamala Harris become the first female Vice President in US history

Here's a list of everything we collected, although we'll only be analyzing a subset for the purpose of this project:

-   A "Maze" task in which participants read sentences word-by-word, choosing between the correct word and a foil at each step. We recorded the response latency associated with each word.
-   \[SKIPPED HERE\]Counterbalancing factors: task order, pronouns used across sentences, etc.
-   \[SKIPPED HERE\] An event expectation task in which participants assigned probabilities to the main election candidates winning the presidency.
-   \[SKIPPED HERE\] A cloze completion task in which participants completed sentences that mentioned the future vice president.
-   \[SKIPPED HERE\] A recall task to double-check that participants were paying attention to the pronouns in the comprehension tasks.
-   \[SKIPPED HERE\] Self-paced reading task in which participants read sentences word-by-word while we recorded the response latency associated with each word.

# Reading raw data

First step is to read the raw data files line by line.

```{r read-raw-data}
#| label: read-raw-data

lines_by_pattern <- function(pattern, raw_data_lines) {
  # Extract lines containing the pattern
  matching_lines <- raw_data_lines[str_detect(raw_data_lines, pattern)]
  
  if (length(matching_lines) == 0) {
    warning(sprintf("No lines matched pattern: %s", pattern))
    return(NULL)
  }

  # Read the matching lines into a dataframe and parse submission time
  tryCatch({
    paste(matching_lines, collapse = "\n") |>
      read_csv(col_names = FALSE, show_col_types = FALSE) |>
      rename(md5 = X2) |>
      mutate(submission_time = as.POSIXct(X1, origin = "1970-01-01", tz = "UTC")) |>
      select(-X1)
  }, error = function(e) {
    warning(sprintf("Error processing pattern '%s': %s", pattern, e$message))
    return(NULL)
  })
}

```

Now we can extract the different types of data from these lines that were recorded throughout the ibex experiment, starting with the Maze task data (our main dependent variable) before turning to demographic data.

# Extracting Maze task data

Our hypothesis tests will focus on the first pronoun in the sentence, so we'll add a column to the dataframe that indicates whether the word is the first pronoun in the sentence. We do want to retain the other words as well for plotting purposes, though.

```{r extract-maze-data}
#| label: extract-maze-data

process_maze_data <- function(raw_lines, data_round) {
  pronouns = c("he", "she", "they",
               "him", "her", "them",
               "his", "her", "their")
  pronouns_regex = paste0("\\b(", paste(pronouns, collapse = "|"), ")\\b")

  lines_by_pattern("0,maze,", raw_lines) |>
    rename(word_no = X8,
           word = X9,
           distractor = X10,
           response_time = X13,
           maze_sens = X14) |>
    group_by(submission_time, md5) |>
    mutate(is_pronoun = word %in% pronouns,
           is_first_pronoun = is_pronoun & word_no == min(word_no[is_pronoun]),
           word_no_relative_to_first_pronoun = word_no - min(word_no[is_pronoun])) |>
    ungroup() |>
    mutate(word = str_replace_all(word, "%2C", ","),
           distractor = str_replace_all(distractor, "%2C", ","),
           maze_sens = str_replace_all(maze_sens, "%2C", ",")) |>
    separate(maze_sens,
             into = c("context_sentence", "target_sentence"),
             sep = "\\.\\s",
             extra = "drop") |>
    mutate(
      context_sentence = paste0(context_sentence, "."),
      target_sentence = paste0(target_sentence, "."),
      target_sentence_frame = str_replace_all(target_sentence,
                                            pronouns_regex,
                                            "____"),
      data_round = data_round  # Add identifier for pre/post election
    ) |>
    select(-starts_with("X"), -is_pronoun) ->
    maze_df

  return(maze_df)
}
```

# Extracting demographic data

We'll use the demographic data later in the Shiny app to explore the hypotheses across different demographic groups and subsets. The main hypothesis tests will average over all of these factors since we have no theoretical reason to suspect any differences.

```{r extract-demographics}
#| label: extract-demographics

process_demographic_data <- function(raw_lines, target_df) {
  demographic_vars <- list(
    list(pattern = ",age,", col = "age"),
    list(pattern = ",Please select your gender.,", col = "gender"),
    list(pattern = ",state,", col = "state"),
    list(pattern = "education you have attained:", col = "education"),
    list(pattern = "political affiliation?", col = "political_affiliation"),
    list(pattern = "citizen of the United States?", col = "citizen"),
    list(pattern = "native speaker of English?", col = "native_speaker"),
    list(pattern = "reside in the United States?", col = "reside_in_the_united_states"),
    list(pattern = ",news,", col = "news_consumption"),
    list(pattern = "<i>prefer</i>", col = "election_outcome_preference")
  )

  # Helper function to extract and join a single demographic variable
  extract_demographic <- function(pattern, col_name, data_lines, target_df) {
    result <- lines_by_pattern(pattern, data_lines)
    
    if (is.null(result)) {
      warning(sprintf("Skipping %s - no data found", col_name))
      return(target_df)
    }
    
    result |>
      rename(!!col_name := X9) |>
      select(-starts_with("X")) |>
      right_join(target_df, by = c("submission_time", "md5")) ->
      result
    
    return(result)
  }

  # Process all demographic variables
  target_df <- reduce(
    demographic_vars,
    function(df, var) {
      extract_demographic(var$pattern, var$col, raw_lines, df)
    },
    .init = target_df
  )

  return(target_df)
}
```

# Process data

Now that we've defined functions for reading both demographic data and Maze task data (the main dependent variable), we can process the data.

```{r process-data}
#| label: process-data

# Read raw data
pre_election_lines <- read_lines(file = here("data", "raw", "pre-election-raw-data.txt"))
post_election_lines <- read_lines(file = here("data", "raw", "post-election-raw-data.txt"))

# Process pre-election data
pre_election_maze <- process_maze_data(pre_election_lines, "pre-election")
pre_election_df <- process_demographic_data(pre_election_lines, pre_election_maze)

# Process post-election data
post_election_maze <- process_maze_data(post_election_lines, "post-election")
post_election_df <- process_demographic_data(post_election_lines, post_election_maze)

# Combine the datasets and renumber participants
bind_rows(pre_election_df, post_election_df) |>
  arrange(submission_time) |>
  group_by(md5,submission_time) |>
  mutate(participant_id = sprintf("Participant %04d", cur_group_id())) |>
  ungroup() |>
  select(-md5) ->
  combined_df

glimpse(combined_df)
skimr::skim(combined_df)

```

# Apply exclusion criteria

Before saving the processed and anonymized data, we want to apply some exclusion criteria. Specifically, we want to exclude participants who: - took more than 10 seconds to read any word in the Maze task (may have tabbed out of the experiment) - took less than 180 milliseconds to read any word in the Maze task (may not have been reading properly)

These are standard exclusion criteria for psycholinguistic response time measures.

```{r apply-exclusion-criteria}
#| label: apply-exclusion-criteria

combined_df |>
  group_by(participant_id) |>
  mutate(max_response_time = max(response_time),
         min_response_time = min(response_time)) |>
  ungroup() |>
  filter(max_response_time < 10000, min_response_time > 180) ->
  combined_df

glimpse(combined_df)
skimr::skim(combined_df)

```

# Save data

```{r save-data}
#| label: save-data

write_rds(combined_df, here("data", "processed", "clean-data.rds"))

```

