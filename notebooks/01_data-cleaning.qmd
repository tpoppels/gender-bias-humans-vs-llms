---
title: "Data Cleaning and Preprocessing"
author: "Till Poppels"
format: 
  html:
    theme: cosmo
    toc: true
    toc-depth: 2
    number-sections: true
execute:
  echo: true
  warning: false
  message: false
  error: true
  cache: false
---

```{r setup}
#| label: setup
#| include: false

library(tidyverse)
library(here)
```

# Overview

The raw data files (gitignored for participant privacy) are Ibex results files and as such don't have a consistent CSV structure (different parts of the results files contain different numbers of columns). We will essentially grep the lines for different types of data and store them in individual data frames that will eventually get glued together into a single dataframe and stored as `data/processed/clean-data.rds`.

We will focus here on the data needed for the main research questions and hypothesis tests:

1.  Whether human readers were surprised to see "she" refer to the next Vice President *before the 2020 election*
2.  Whether those expectations were reversed *after the 2020 election*, which saw Kamala Harris become the first female Vice President in US history

Here's a list of everything we collected, although we'll only be analyzing a subset for the purpose of this project:

-   A "Maze" task in which participants read sentences word-by-word, choosing between the correct word and a foil at each step. We recorded the response latency associated with each word.
-   \[SKIPPED HERE\]Counterbalancing factors: task order, pronouns used across sentences, etc.
-   \[SKIPPED HERE\] An event expectation task in which participants assigned probabilities to the main election candidates winning the presidency.
-   \[SKIPPED HERE\] A cloze completion task in which participants completed sentences that mentioned the future vice president.
-   \[SKIPPED HERE\] A recall task to double-check that participants were paying attention to the pronouns in the comprehension tasks.
-   \[SKIPPED HERE\] Self-paced reading task in which participants read sentences word-by-word while we recorded the response latency associated with each word.

# Reading raw data

First step is to read the raw data files line by line.

```{r read-raw-data}
#| label: read-raw-data

lines_by_pattern <- function(pattern, raw_data_lines) {
  # Extract lines containing the pattern
  matching_lines <- raw_data_lines[str_detect(raw_data_lines, pattern)]
  
  if (length(matching_lines) == 0) {
    warning(sprintf("No lines matched pattern: %s", pattern))
    return(NULL)
  }

  # Read the matching lines into a dataframe, parse submission time,
  # and anonymize participant IDs
  tryCatch({
    paste(matching_lines, collapse = "\n") |>
      read_csv(col_names = FALSE, show_col_types = FALSE) |>
      rename(md5 = X2) |>
      mutate(submission_time = as.POSIXct(X1, origin = "1970-01-01", tz = "UTC")) |>
      arrange(submission_time, md5) |>
      mutate(n_digits = ceiling(log10(n_distinct(paste(submission_time, md5))))) |>
      group_by(submission_time, md5) |>
      mutate(sid = sprintf("Sub%0*d", n_digits, cur_group_id())) |>
      ungroup() |>
      select(-X1, -n_digits, -md5)
  }, error = function(e) {
    warning(sprintf("Error processing pattern '%s': %s", pattern, e$message))
    return(NULL)
  })
}

pre_election_lines <- read_lines(file = here("data", "raw", "pre-election-raw-data.txt"))
post_election_lines <- read_lines(file = here("data", "raw", "post-election-raw-data.txt"))

```

Now we can extract the different types of data from these lines that were recorded throughout the ibex experiment, starting with the Maze task data (our main dependent variable) before turning to demographic data.

# Extracting Maze task data

Our hypothesis tests will focus on the first pronoun in the sentence, so we'll add a column to the dataframe that indicates whether the word is the first pronoun in the sentence. We do want to retain the other words as well for plotting purposes, though.
```{r extract-maze-data}
#| label: extract-maze-data
pronouns = c("he", "she", "they",
             "him", "her", "them",
             "his", "her", "their")
pronouns_regex = paste0("\\b(", paste(pronouns, collapse = "|"), ")\\b")

lines_by_pattern("0,maze,", pre_election_lines) |>
  rename(word_no = X8,
         word = X9,
         distractor = X10,
         response_time = X13,
         maze_sens = X14) |>
  group_by(sid) |>
  mutate(is_pronoun = word %in% pronouns,
         is_first_pronoun = is_pronoun & word_no == min(word_no[is_pronoun]),
         word_no_relative_to_first_pronoun = word_no - min(word_no[is_pronoun])) |>
  ungroup() |>
  mutate(word = str_replace_all(word, "%2C", ","),
         distractor = str_replace_all(distractor, "%2C", ","),
         maze_sens = str_replace_all(maze_sens, "%2C", ",")) |>
  separate(maze_sens,
           into = c("context_sentence", "target_sentence"),
           sep = "\\.\\s",
           extra = "drop") |>
  mutate(
    context_sentence = paste0(context_sentence, "."),
    target_sentence = paste0(target_sentence, "."),
    target_sentence_frame = str_replace_all(target_sentence,
                                            pronouns_regex,
                                            "____")) |>
  select(-starts_with("X"), -is_pronoun) ->
  pre_election_df

```

# Extracting demographic data

We'll use the demographic data later in the Shiny app to explore the hypotheses across different demographic groups and subsets. The main hypothesis tests will average over all of these factors since we have no theoretical reason to suspect any differences.

```{r extract-demographics}
#| label: extract-demographics

# Define the demographic variables to extract
demographic_vars <- list(
  list(pattern = ",age,", col = "age"),
  list(pattern = ",Please select your gender.,", col = "gender"),
  list(pattern = ",state,", col = "state"),
  list(pattern = "education you have attained:", col = "education"),
  list(pattern = "political affiliation?", col = "political_affiliation"),
  list(pattern = "citizen of the United States?", col = "citizen"),
  list(pattern = "native speaker of English?", col = "native_speaker"),
  list(pattern = "reside in the United States?", col = "reside_in_the_united_states"),
  list(pattern = ",news,", col = "news_consumption"),
  list(pattern = "<i>prefer</i>", col = "election_outcome_preference")
)

for (var in demographic_vars) {
  cat("\nChecking pattern:", var$pattern, "\n")
  result <- lines_by_pattern(var$pattern, pre_election_lines)
  cat("Result:", nrow(result), "rows\n")
}

# Helper function to extract and join a single demographic variable
extract_demographic <- function(pattern, col_name, data_lines, target_df) {
  
  result <- lines_by_pattern(pattern, data_lines)
  
  if (is.null(result)) {
    warning(sprintf("Skipping %s - no data found", col_name))
    return(target_df)
  }
  
  result |>
    rename(!!col_name := X9) |>
    select(-starts_with("X"), -submission_time) |>
    right_join(target_df, by = c("sid")) ->
    result
  
  return(result)
}

# Extract all demographic variables
pre_election_df <- reduce(
  demographic_vars,
  function(df, var) {
    extract_demographic(var$pattern, var$col, pre_election_lines, df)
  },
  .init = pre_election_df
)

# Check the final structure
glimpse(pre_election_df)
```

